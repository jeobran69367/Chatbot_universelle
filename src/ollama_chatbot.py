"""
Chatbot int√©grant Ollama pour des r√©ponses intelligentes locales.
Cette version remplace OpenAI par Ollama pour une solution enti√®rement locale.
"""

import logging
import json
import requests
from typing import Dict, List, Optional, Any
from datetime import datetime

try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

import os
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au chemin pour les imports
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.append(str(parent_dir))

from config.settings import OLLAMA_HOST, OLLAMA_MODEL

class OllamaChatBot:
    """
    Chatbot intelligent utilisant Ollama avec RAG (Retrieval Augmented Generation).
    """
    
    def __init__(self, model: str = OLLAMA_MODEL, host: str = OLLAMA_HOST):
        """
        Initialise le chatbot Ollama.
        
        Args:
            model: Mod√®le Ollama √† utiliser (ex: 'llama3.1', 'mistral', 'codellama')
            host: Adresse du serveur Ollama
        """
        self.model = model
        self.host = host
        self.conversation_history: List[Dict[str, str]] = []
        self.logger = logging.getLogger(__name__)
        
        # V√©rifier si Ollama est disponible
        self._check_ollama_availability()
        
        # System prompt pour le chatbot
        self.system_prompt = """
        Vous √™tes un assistant IA intelligent et serviable qui aide les utilisateurs en r√©pondant √† leurs questions 
        en utilisant les informations provenant de sites web qui ont √©t√© analys√©s et index√©s.
        
        Instructions importantes:
        1. Utilisez principalement les informations fournies dans le contexte pour r√©pondre aux questions
        2. Si l'information n'est pas disponible dans le contexte, indiquez-le clairement
        3. Soyez pr√©cis et informatif dans vos r√©ponses
        4. Citez la source (URL) quand c'est pertinent
        5. R√©pondez en fran√ßais de mani√®re naturelle et conversationnelle
        6. Si vous n'√™tes pas s√ªr d'une information, dites-le explicitement
        
        Contexte des documents analys√©s:
        {context}
        
        Conversation pr√©c√©dente:
        {conversation_history}
        """
    
    def _check_ollama_availability(self):
        """V√©rifie si Ollama est disponible."""
        try:
            response = requests.get(f"{self.host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model not in available_models:
                    self.logger.warning(f"Mod√®le {self.model} non trouv√©. Mod√®les disponibles: {available_models}")
                    if available_models:
                        self.model = available_models[0]
                        self.logger.info(f"Utilisation du mod√®le: {self.model}")
                
                self.logger.info(f"Ollama disponible avec le mod√®le: {self.model}")
            else:
                raise Exception("Serveur Ollama non accessible")
                
        except Exception as e:
            error_msg = f"Erreur de connexion √† Ollama: {e}"
            self.logger.error(error_msg)
            raise Exception(f"Assurez-vous qu'Ollama est install√© et lanc√©: {error_msg}")
    
    def _call_ollama_api(self, prompt: str) -> str:
        """Appelle l'API Ollama pour g√©n√©rer une r√©ponse."""
        try:
            if OLLAMA_AVAILABLE:
                # Utiliser la biblioth√®que ollama si disponible
                response = ollama.chat(
                    model=self.model,
                    messages=[{'role': 'user', 'content': prompt}],
                    stream=False
                )
                return response['message']['content']
            else:
                # Utiliser l'API REST directement
                payload = {
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_predict": 1000
                    }
                }
                
                response = requests.post(
                    f"{self.host}/api/generate",
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    return response.json()["response"]
                else:
                    raise Exception(f"Erreur API Ollama: {response.status_code} - {response.text}")
                    
        except Exception as e:
            self.logger.error(f"Erreur lors de l'appel √† Ollama: {e}")
            raise
    
    def format_context_from_search_results(self, search_results: List[Dict[str, Any]]) -> str:
        """
        Formate les r√©sultats de recherche en contexte pour le prompt.
        
        Args:
            search_results: Liste des r√©sultats de recherche de la base vectorielle
            
        Returns:
            Cha√Æne de contexte format√©e
        """
        if not search_results:
            return "Aucun contexte pertinent trouv√© dans les documents analys√©s."
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            content = result.get('content', '')
            metadata = result.get('metadata', {})
            source_url = metadata.get('source_url', 'Source inconnue')
            title = metadata.get('title', 'Titre inconnu')
            
            context_part = f"""
Document {i}:
Titre: {title}
Source: {source_url}
Contenu: {content}
---
"""
            context_parts.append(context_part)
        
        return '\n'.join(context_parts)
    
    def format_conversation_history(self) -> str:
        """
        Formate l'historique de conversation pour le prompt.
        
        Returns:
            Historique de conversation format√©
        """
        if not self.conversation_history:
            return "Aucune conversation pr√©c√©dente."
        
        history_parts = []
        # Garder seulement les derniers √©changes pour √©viter les limites
        recent_history = self.conversation_history[-6:]  # 3 derniers √©changes
        
        for message in recent_history:
            role = message['role']
            content = message['content']
            if role == 'user':
                history_parts.append(f"Utilisateur: {content}")
            elif role == 'assistant':
                history_parts.append(f"Assistant: {content}")
        
        return '\n'.join(history_parts)
    
    def generate_response(self, 
                         user_question: str, 
                         search_results: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse utilisant Ollama avec RAG.
        
        Args:
            user_question: Question de l'utilisateur
            search_results: R√©sultats de recherche pertinents de la base vectorielle
            
        Returns:
            Dictionnaire contenant la r√©ponse et les m√©tadonn√©es
        """
        try:
            # Formater le contexte √† partir des r√©sultats de recherche
            context = ""
            if search_results:
                context = self.format_context_from_search_results(search_results)
            else:
                context = "Aucun contexte sp√©cifique fourni."
            
            # Formater l'historique de conversation
            conversation_history = self.format_conversation_history()
            
            # Cr√©er le prompt avec le contexte
            full_prompt = self.system_prompt.format(
                context=context,
                conversation_history=conversation_history
            )
            
            # Ajouter la question de l'utilisateur
            full_prompt += f"\n\nQuestion de l'utilisateur: {user_question}\n\nR√©ponse:"
            
            # Enregistrer la requ√™te
            self.logger.info(f"G√©n√©ration de r√©ponse pour: {user_question[:100]}...")
            
            # Faire l'appel √† Ollama
            assistant_response = self._call_ollama_api(full_prompt)
            
            # Mettre √† jour l'historique de conversation
            self.conversation_history.append({"role": "user", "content": user_question})
            self.conversation_history.append({"role": "assistant", "content": assistant_response})
            
            # Garder l'historique g√©rable
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            # Pr√©parer les donn√©es de r√©ponse
            response_data = {
                "response": assistant_response,
                "model": self.model,
                "timestamp": datetime.now().isoformat(),
                "sources_used": len(search_results) if search_results else 0,
                "host": self.host
            }
            
            self.logger.info("R√©ponse g√©n√©r√©e avec succ√®s")
            
            return response_data
            
        except Exception as e:
            error_message = f"Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}"
            self.logger.error(error_message)
            
            return {
                "response": error_message,
                "error": True,
                "timestamp": datetime.now().isoformat(),
                "model": self.model
            }
    
    def clear_conversation_history(self):
        """Efface l'historique de conversation."""
        self.conversation_history.clear()
        self.logger.info("Historique de conversation effac√©")
    
    def get_conversation_history(self) -> List[Dict[str, str]]:
        """
        R√©cup√®re l'historique de conversation actuel.
        
        Returns:
            Liste des messages de conversation
        """
        return self.conversation_history.copy()
    
    def set_system_prompt(self, new_prompt: str = None, style: str = None):
        """
        Met √† jour le prompt syst√®me.
        
        Args:
            new_prompt: Nouveau prompt syst√®me direct
            style: Style de prompt √† appliquer
        """
        if style:
            # D√©finir les styles de prompts disponibles
            prompt_styles = {
                "Assistant Professionnel": """
                Vous √™tes un assistant IA professionnel et efficace qui aide les utilisateurs en r√©pondant √† leurs questions 
                de mani√®re pr√©cise et structur√©e. Utilisez les informations des documents analys√©s pour fournir des r√©ponses 
                compl√®tes et bien organis√©es.

                Instructions:
                1. Structurez vos r√©ponses clairement avec des points ou des sections
                2. Citez les sources quand c'est pertinent
                3. Soyez concis mais complet
                4. Utilisez un ton professionnel et courtois
                
                Contexte: {context}
                Conversation pr√©c√©dente: {conversation_history}
                """,
                
                "Expert Technique": """
                Vous √™tes un expert technique qui fournit des analyses approfondies et des explications d√©taill√©es. 
                Utilisez les informations techniques des documents pour donner des r√©ponses pr√©cises et expertes.

                Instructions:
                1. Fournissez des d√©tails techniques pertinents
                2. Expliquez les concepts complexes clairement
                3. Mentionnez les limitations ou consid√©rations importantes
                4. Citez les sources techniques
                
                Contexte: {context}
                Conversation pr√©c√©dente: {conversation_history}
                """,
                
                "Guide P√©dagogique": """
                Vous √™tes un guide p√©dagogique qui aide √† comprendre et apprendre. Expliquez les concepts de mani√®re 
                accessible et progressive, en utilisant des exemples et des analogies quand c'est utile.

                Instructions:
                1. Expliquez √©tape par √©tape
                2. Utilisez des exemples concrets
                3. Adaptez le niveau d'explication √† l'utilisateur
                4. Encouragez l'apprentissage progressif
                
                Contexte: {context}
                Conversation pr√©c√©dente: {conversation_history}
                """,
                
                "Conseiller Bienveillant": """
                Vous √™tes un conseiller bienveillant qui aide avec empathie et compr√©hension. Fournissez des conseils 
                r√©fl√©chis et du support en utilisant les informations disponibles.

                Instructions:
                1. Montrez de l'empathie dans vos r√©ponses
                2. Fournissez des conseils pratiques
                3. Soyez encourageant et supportif
                4. Respectez les pr√©occupations de l'utilisateur
                
                Contexte: {context}
                Conversation pr√©c√©dente: {conversation_history}
                """,
                
                "Analyste Concis": """
                Vous √™tes un analyste qui fournit des r√©ponses concises et directes. Allez droit au but avec les 
                informations les plus importantes tir√©es des documents.

                Instructions:
                1. Soyez direct et concis
                2. Priorisez les informations cl√©s
                3. √âvitez les d√©tails superflus
                4. Structurez vos r√©ponses en points essentiels
                
                Contexte: {context}
                Conversation pr√©c√©dente: {conversation_history}
                """
            }
            
            if style in prompt_styles:
                self.system_prompt = prompt_styles[style]
                self.logger.info(f"Style de prompt appliqu√©: {style}")
            else:
                self.logger.warning(f"Style inconnu: {style}. Styles disponibles: {list(prompt_styles.keys())}")
        elif new_prompt:
            self.system_prompt = new_prompt
            self.logger.info("Prompt syst√®me mis √† jour directement")
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        R√©cup√®re des informations sur la configuration actuelle du mod√®le.
        
        Returns:
            Dictionnaire avec les informations du mod√®le
        """
        return {
            "model": self.model,
            "host": self.host,
            "conversation_length": len(self.conversation_history),
            "ollama_available": OLLAMA_AVAILABLE
        }
    
    def list_available_models(self) -> List[str]:
        """Liste les mod√®les Ollama disponibles."""
        try:
            response = requests.get(f"{self.host}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            else:
                return []
        except Exception as e:
            self.logger.error(f"Erreur lors de la r√©cup√©ration des mod√®les: {e}")
            return []
    
    def list_available_prompt_styles(self) -> List[str]:
        """Liste les styles de prompt disponibles."""
        # Styles de r√©ponse disponibles pour le chatbot
        available_styles = [
            "Assistant Professionnel",
            "Expert Technique", 
            "Guide P√©dagogique",
            "Conseiller Bienveillant",
            "Analyste Concis"
        ]
        return available_styles

class ResponseFormatter:
    """Classe utilitaire pour formater les r√©ponses du chatbot."""
    
    @staticmethod
    def format_for_streamlit(response_data: Dict[str, Any]) -> str:
        """
        Formate la r√©ponse pour l'affichage Streamlit.
        
        Args:
            response_data: Donn√©es de r√©ponse du chatbot
            
        Returns:
            Cha√Æne de r√©ponse format√©e
        """
        response = response_data.get("response", "")
        
        if response_data.get("error"):
            return f"‚ùå **Erreur**: {response}"
        
        # Ajouter un pied de page avec m√©tadonn√©es si disponible
        footer_parts = []
        
        if "sources_used" in response_data and response_data["sources_used"] > 0:
            footer_parts.append(f"üìö Sources consult√©es: {response_data['sources_used']}")
        
        if "model" in response_data:
            footer_parts.append(f"ü§ñ Mod√®le: {response_data['model']}")
        
        if footer_parts:
            footer = "\n\n---\n*" + " | ".join(footer_parts) + "*"
            return response + footer
        
        return response
    
    @staticmethod
    def extract_sources(search_results: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        Extrait les informations de source des r√©sultats de recherche.
        
        Args:
            search_results: R√©sultats de recherche de la base vectorielle
            
        Returns:
            Liste des informations de source
        """
        sources = []
        for result in search_results:
            metadata = result.get('metadata', {})
            source = {
                'title': metadata.get('title', 'Titre inconnu'),
                'url': metadata.get('source_url', '#'),
                'snippet': result.get('content', '')[:200] + '...' if len(result.get('content', '')) > 200 else result.get('content', '')
            }
            sources.append(source)
        
        return sources

# Exemple d'utilisation
if __name__ == "__main__":
    # Initialiser le chatbot
    chatbot = OllamaChatBot()
    
    # Exemples de r√©sultats de recherche (viendraient de la base vectorielle)
    example_search_results = [
        {
            'content': "L'intelligence artificielle est une technologie r√©volutionnaire...",
            'metadata': {
                'source_url': 'https://example.com/ai-article',
                'title': 'Introduction √† l\'IA',
                'chunk_index': 0
            }
        }
    ]
    
    # G√©n√©rer une r√©ponse
    response = chatbot.generate_response(
        user_question="Qu'est-ce que l'intelligence artificielle ?",
        search_results=example_search_results
    )
    
    print("R√©ponse:", response["response"])
    print("Mod√®le:", response.get("model", "N/A"))
