# ü§ñ Chatbot Web Scraper Intelligent

Un chatbot intelligent capable de scraper des sites web, vectoriser le contenu et r√©pondre aux questions en utilisant **Ollama** (LLaMA, Mistral, etc.) et des techniques de RAG (Retrieval Augmented Generation) - **100% local et priv√©**.

## ‚ú® Fonctionnalit√©s

- üï∑Ô∏è **Web Scraping Avanc√©**: Scrape automatiquement un site web et toutes ses pages li√©es
- üß† **Intelligence Artificielle Locale**: Utilise Ollama avec LLaMA 3.1, Mistral ou d'autres mod√®les open-source
- üìä **Base de Donn√©es Vectorielle**: Stocke et recherche efficacement dans le contenu scrap√©
- üé® **Interface Intuitive**: Interface web moderne avec Streamlit
- üîç **Recherche S√©mantique**: Trouve le contenu pertinent pour chaque question
- üí¨ **Conversation Contextuelle**: Maintient l'historique des conversations
- üîí **100% Priv√©**: Toutes les donn√©es restent en local, aucune API externe requise
- üé≠ **Prompts Personnalisables**: Styles de r√©ponse adaptables (d√©faut, expert, casual)

## üõ†Ô∏è Technologies Utilis√©es

- **Frontend**: Streamlit
- **AI/ML**: Ollama (LLaMA, Mistral, etc.), Sentence Transformers
- **Web Scraping**: BeautifulSoup, Selenium, Requests
- **Base de Donn√©es Vectorielle**: ChromaDB ou FAISS
- **Traitement de Texte**: NLTK, spaCy, LangChain
- **Langages**: Python 3.8+

## üì¶ Installation

### 1. Cloner le projet
```bash
git clone <votre-repo>
cd Model
```

### 2. Cr√©er un environnement virtuel
```bash
python -m venv .venv
source .venv/bin/activate  # Sur macOS/Linux
# ou
.venv\Scripts\activate  # Sur Windows
```

### 3. Installer les d√©pendances
```bash
pip install -r requirements.txt
```

### 4. Installer et configurer Ollama

**Sur macOS:**
```bash
# Installer Ollama
brew install ollama

# Ou t√©l√©charger depuis https://ollama.ai/download
curl -fsSL https://ollama.ai/install.sh | sh
```

**Sur Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Sur Windows:**
- T√©l√©chargez l'installateur depuis [https://ollama.ai/download](https://ollama.ai/download)
- Ex√©cutez l'installateur et suivez les instructions

### 5. D√©marrer Ollama et t√©l√©charger un mod√®le
```bash
# D√©marrer le service Ollama (si pas auto-d√©marr√©)
ollama serve

# Dans un nouveau terminal, t√©l√©charger le mod√®le LLaMA 3.1
ollama pull llama3.1:latest

# Optionnel: T√©l√©charger des mod√®les plus l√©gers
ollama pull qwen2:0.5b    # Mod√®le tr√®s l√©ger (500MB)
ollama pull mistral:7b    # Mod√®le Mistral (4.1GB)

# V√©rifier les mod√®les install√©s
ollama list
```

### 6. Configuration des variables d'environnement
Cr√©ez un fichier `.env` √† la racine du projet :
```bash
cp .env.example .env
```

Modifiez le fichier `.env` avec vos configurations :
```env
# Configuration Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:latest

# Configuration Streamlit (optionnel)
STREAMLIT_THEME_BASE=light
STREAMLIT_THEME_PRIMARY_COLOR=#FF6B6B
STREAMLIT_THEME_BACKGROUND_COLOR=#FFFFFF
```

### 7. Lancer l'application

**üîç V√©rification rapide avant de commencer :**
```bash
# V√©rifier qu'Ollama fonctionne
ollama list

# Tester un mod√®le
ollama run llama3.1:latest "Bonjour, comment √ßa va ?"

# V√©rifier l'environnement Python
source .venv/bin/activate
python -c "import streamlit, chromadb, sentence_transformers; print('‚úÖ Tous les modules OK')"
```

**Option 1 : Avec l'environnement virtuel activ√©**
```bash
source .venv/bin/activate  # Sur macOS/Linux
# ou
.venv\Scripts\activate  # Sur Windows
streamlit run app.py
```

**Option 2 : Directement avec le chemin complet**
```bash
./.venv/bin/streamlit run app.py  # Sur macOS/Linux
# ou
.venv\Scripts\streamlit run app.py  # Sur Windows
```

**Option 3 : Script de d√©marrage automatique**
```bash
chmod +x start_chatbot.sh
./start_chatbot.sh
```

**üöÄ Acc√®s √† l'application :**
- Interface web : http://localhost:8501
- L'application se lance automatiquement dans votre navigateur

## üöÄ Utilisation

### Interface Web

1. **Scraping d'un Site Web**:
   - Entrez l'URL du site dans la sidebar
   - Configurez le nombre maximum de pages
   - Choisissez d'utiliser Selenium si n√©cessaire
   - Cliquez sur "Lancer le Scraping"

2. **Poser des Questions**:
   - Tapez votre question dans la zone de texte
   - Le chatbot utilisera le contenu scrap√© pour r√©pondre
   - Les sources utilis√©es sont affich√©es dans la sidebar

3. **Gestion des Donn√©es**:
   - Consultez l'onglet "Donn√©es" pour voir les informations scrap√©es
   - Chargez des donn√©es existantes si disponibles

### Utilisation Programmatique

```python
from src.web_scraper import WebScraper
from src.vector_database import VectorDatabase
from src.chatbot import ChatBot

# 1. Scraper un site web
scraper = WebScraper()
pages = scraper.scrape_website("https://example.com", max_pages=20)

# 2. Cr√©er la base de donn√©es vectorielle
vector_db = VectorDatabase("chroma")
vector_db.add_documents_from_scraped_data(pages)

# 3. Initialiser le chatbot
chatbot = ChatBot()

# 4. Poser une question
search_results = vector_db.search_similar("ma question", n_results=5)
response = chatbot.generate_response("ma question", search_results)
print(response["response"])
```

## üìÅ Structure du Projet

```
Model/
‚îú‚îÄ‚îÄ app.py                 # Interface Streamlit principale
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ .env                  # Variables d'environnement
‚îú‚îÄ‚îÄ README.md             # Documentation
‚îú‚îÄ‚îÄ start_chatbot.sh      # Script de d√©marrage automatique
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ settings.py       # Configuration de l'application
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py        # Configuration des prompts syst√®me
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ web_scraper.py    # Module de scraping web
‚îÇ   ‚îú‚îÄ‚îÄ vector_database.py # Base de donn√©es vectorielle
‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py        # Int√©gration Ollama
‚îÇ   ‚îî‚îÄ‚îÄ robust_vector_db.py # Solution robuste ChromaDB
‚îî‚îÄ‚îÄ data/                 # Donn√©es scrap√©es et mod√®les
    ‚îú‚îÄ‚îÄ scraped/          # Donn√©es JSON brutes
    ‚îú‚îÄ‚îÄ embeddings/       # Base de donn√©es vectorielle
    ‚îî‚îÄ‚îÄ models/           # Mod√®les t√©l√©charg√©s
```

## ‚öôÔ∏è Configuration Avanc√©e

### Param√®tres de Scraping

Dans `config/settings.py` :

```python
MAX_PAGES_PER_SITE = 100    # Limite de pages par site
REQUEST_DELAY = 1           # D√©lai entre les requ√™tes (secondes)
MAX_RETRIES = 3             # Nombre de tentatives
TIMEOUT = 30                # Timeout des requ√™tes
```

### Param√®tres de Vectorisation

```python
CHUNK_SIZE = 1000           # Taille des chunks de texte
CHUNK_OVERLAP = 200         # Chevauchement entre chunks
SIMILARITY_THRESHOLD = 0.7  # Seuil de similarit√©
```

### Mod√®les Ollama

```python
# Configuration dans .env ou directement
OLLAMA_MODEL = "llama3.1:latest"  # Mod√®le par d√©faut
OLLAMA_MODEL = "mistral:7b"       # Mod√®le Mistral
OLLAMA_MODEL = "qwen2:0.5b"       # Mod√®le l√©ger pour tests
```

**Mod√®les recommand√©s :**
- `llama3.1:latest` - Excellent √©quilibre performance/qualit√© (4.6GB)
- `mistral:7b` - Tr√®s bon pour le fran√ßais (4.1GB)
- `qwen2:0.5b` - Ultra-l√©ger pour d√©veloppement (500MB)
- `codellama:latest` - Sp√©cialis√© en programmation (3.8GB)

## üîß Personnalisation

### Ajouter de Nouvelles Sources de Donn√©es

```python
# Dans web_scraper.py
def scrape_api_data(api_url):
    # Impl√©mentez votre logique de scraping d'API
    pass
```

### Modifier le Prompt Syst√®me

**Option 1: Via l'interface Streamlit**
- Utilisez le s√©lecteur "Style de r√©ponse" dans la sidebar
- Styles disponibles: `default`, `expert`, `casual`

**Option 2: Par programmation**
```python
# Dans chatbot.py
chatbot = ChatBot(prompt_style="expert")  # Style au d√©marrage

# Ou changer dynamiquement
chatbot.set_system_prompt(style="casual")
chatbot.set_system_prompt("Votre prompt personnalis√©...")
```

**Option 3: Personnalisation avanc√©e**
Modifiez le fichier `config/prompts.py` pour ajouter vos propres styles de prompts.

### Changer de Base de Donn√©es Vectorielle

```python
# Utiliser FAISS au lieu de ChromaDB
vector_db = VectorDatabase("faiss")
```

## üêõ D√©pannage

### Erreurs Communes

1. **Erreur de connexion Ollama** : 
   - V√©rifiez qu'Ollama est d√©marr√© : `ollama serve`
   - V√©rifiez le mod√®le : `ollama list`
   - Red√©marrez Ollama si n√©cessaire

2. **Mod√®le non trouv√©** :
   ```bash
   ollama pull llama3.1:latest
   ```

3. **Erreur de Selenium** : 
   - Installez ChromeDriver ou utilisez `use_selenium=False`
   - Sur macOS : `brew install chromedriver`

4. **Probl√®mes de m√©moire** : 
   - R√©duisez `MAX_PAGES_PER_SITE` ou `CHUNK_SIZE`
   - Utilisez un mod√®le plus l√©ger comme `qwen2:0.5b`

5. **Port 11434 occup√©** :
   ```bash
   # Arr√™ter Ollama
   killall ollama
   # Red√©marrer
   ollama serve
   ```

### Logs de Debug

Pour activer les logs d√©taill√©s :

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìä M√©triques et Performance

### Optimisations Recommand√©es

- Utilisez FAISS pour de gros volumes de donn√©es
- Ajustez `CHUNK_SIZE` selon votre contenu
- Utilisez un cache Redis pour les embeddings fr√©quents
- Impl√©mentez une pagination pour l'interface web

### Limites Actuelles

- Maximum 100 pages par site (configurable)
- Limite de contexte Ollama (variable selon le mod√®le)
- Pas de mise √† jour automatique du contenu
- N√©cessite Ollama en local (peut √™tre d√©ploy√© en remote)

## ü§ù Contribution

1. Fork le projet
2. Cr√©ez votre branche (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## üôè Remerciements

- **Ollama** pour l'infrastructure d'IA locale
- **Meta AI** pour les mod√®les LLaMA
- **Mistral AI** pour les mod√®les Mistral
- L'√©quipe **Streamlit** pour l'interface utilisateur
- La communaut√© **ChromaDB** pour la base de donn√©es vectorielle
- Tous les contributeurs des librairies open-source utilis√©es

## üìû Support

Pour des questions ou du support :
- Ouvrez une issue sur GitHub
- Consultez la documentation des modules individuels
- V√©rifiez les logs d'application

---

**D√©velopp√© avec ‚ù§Ô∏è pour l'intelligence artificielle et la recherche s√©mantique.**
