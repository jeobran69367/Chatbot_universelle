#!/usr/bin/env python3
"""
Diagnostic et solution pour le problÃ¨me PyTorch/sentence-transformers.
Ce script identifie la cause et propose des solutions.
"""

import sys
import os
from pathlib import Path

def diagnose_pytorch_issue():
    """Diagnostic du problÃ¨me PyTorch."""
    print("ğŸ” Diagnostic du problÃ¨me PyTorch/sentence-transformers")
    print("=" * 60)
    
    try:
        import torch
        print(f"âœ… PyTorch installÃ©: {torch.__version__}")
        
        # VÃ©rifier CUDA
        if torch.cuda.is_available():
            print(f"âœ… CUDA disponible: {torch.version.cuda}")
        else:
            print("âš ï¸ CUDA non disponible (CPU seulement)")
        
        # VÃ©rifier MPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("âœ… MPS (Apple Silicon) disponible")
        else:
            print("âš ï¸ MPS non disponible")
            
        # Test simple de tensor
        x = torch.rand(2, 3)
        print(f"âœ… Test tensor basique rÃ©ussi: {x.shape}")
        
    except Exception as e:
        print(f"âŒ Erreur PyTorch: {e}")
        return False
    
    try:
        from sentence_transformers import SentenceTransformer
        print("âœ… sentence-transformers importÃ©")
        
        # Test avec un modÃ¨le trÃ¨s lÃ©ger
        print("ğŸ§ª Test de chargement d'un modÃ¨le lÃ©ger...")
        model = SentenceTransformer('all-MiniLM-L6-v2')
        print("âœ… ModÃ¨le sentence-transformers chargÃ©")
        
        # Test d'encoding simple
        sentences = ["Test simple"]
        embeddings = model.encode(sentences)
        print(f"âœ… Encoding rÃ©ussi: {embeddings.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur sentence-transformers: {e}")
        return False

def create_alternative_embedding():
    """CrÃ©er une alternative d'embedding sans sentence-transformers."""
    print("\nğŸ”§ CrÃ©ation d'une solution alternative...")
    
    alternative_code = '''"""
Alternative d'embedding sans sentence-transformers.
Utilise des mÃ©thodes plus simples et robustes.
"""

import numpy as np
import re
from typing import List
import hashlib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class SimpleEmbeddingGenerator:
    """GÃ©nÃ©rateur d'embeddings simple sans dÃ©pendances lourdes."""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(
            max_features=384,  # Dimension compatible avec sentence-transformers
            stop_words='english',
            lowercase=True,
            ngram_range=(1, 2)
        )
        self.is_fitted = False
    
    def preprocess_text(self, text: str) -> str:
        """PrÃ©processing simple du texte."""
        # Nettoyer le texte
        text = re.sub(r'[^a-zA-ZÃ€-Ã¿0-9\\s]', ' ', text)
        text = re.sub(r'\\s+', ' ', text).strip()
        return text.lower()
    
    def fit_texts(self, texts: List[str]):
        """EntraÃ®ner le vectoriseur sur un corpus de textes."""
        processed_texts = [self.preprocess_text(text) for text in texts]
        self.vectorizer.fit(processed_texts)
        self.is_fitted = True
    
    def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """GÃ©nÃ©rer des embeddings pour une liste de textes."""
        processed_texts = [self.preprocess_text(text) for text in texts]
        
        if not self.is_fitted:
            # Auto-fit si pas encore fait
            self.fit_texts(processed_texts)
        
        # GÃ©nÃ©rer les vecteurs TF-IDF
        embeddings = self.vectorizer.transform(processed_texts).toarray()
        
        # Normaliser les vecteurs
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1  # Ã‰viter la division par zÃ©ro
        normalized_embeddings = embeddings / norms
        
        return normalized_embeddings
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """Interface compatible avec sentence-transformers."""
        return self.generate_embeddings(texts)

class FallbackEmbedding:
    """Embedding de fallback ultra-simple."""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """GÃ©nÃ¨re des embeddings dÃ©terministes basÃ©s sur le hash du texte."""
        embeddings = []
        
        for text in texts:
            # Utiliser le hash du texte pour gÃ©nÃ©rer un vecteur reproductible
            text_hash = hashlib.md5(text.encode()).hexdigest()
            
            # Convertir le hash en vecteur de dimension fixe
            vector = []
            for i in range(0, min(len(text_hash), self.dimension // 16)):
                chunk = text_hash[i*2:(i+1)*2]
                vector.extend([int(chunk, 16) / 255.0] * 16)
            
            # ComplÃ©ter ou tronquer Ã  la bonne dimension
            if len(vector) < self.dimension:
                vector.extend([0.0] * (self.dimension - len(vector)))
            else:
                vector = vector[:self.dimension]
            
            # Normaliser le vecteur
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = np.array(vector) / norm
            else:
                vector = np.random.randn(self.dimension)
                vector = vector / np.linalg.norm(vector)
            
            embeddings.append(vector)
        
        return np.array(embeddings)
'''
    
    # Ã‰crire le code alternatif
    alternative_path = Path(__file__).parent.parent / "src" / "alternative_embedding.py"
    with open(alternative_path, 'w', encoding='utf-8') as f:
        f.write(alternative_code)
    
    print(f"âœ… Solution alternative crÃ©Ã©e: {alternative_path}")
    return alternative_path

def main():
    success = diagnose_pytorch_issue()
    
    if not success:
        print("\nğŸ› ï¸ CrÃ©ation d'une solution de contournement...")
        alternative_path = create_alternative_embedding()
        
        print(f"""
âš ï¸ PyTorch/sentence-transformers a des problÃ¨mes sur votre systÃ¨me.

âœ… SOLUTION CRÃ‰Ã‰E: {alternative_path}

ğŸ“ Pour utiliser la solution alternative:

1. Modifiez src/vector_database.py pour utiliser SimpleEmbeddingGenerator
2. Ou installez des versions compatibles:
   
   pip uninstall torch sentence-transformers
   pip install torch==2.0.1 sentence-transformers==2.2.2
   
3. Ou utilisez ChromaDB avec son embedding par dÃ©faut (recommandÃ©)

ğŸ¯ La solution la plus simple: utiliser ChromaDB sans embedding personnalisÃ©.
""")
    else:
        print("\nğŸ‰ PyTorch et sentence-transformers fonctionnent correctement!")

if __name__ == "__main__":
    main()
