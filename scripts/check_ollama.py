#!/usr/bin/env python3
"""
Script de v√©rification et test d'Ollama pour le chatbot.
Ce script v√©rifie que Ollama est install√©, configur√© et fonctionne correctement.
"""

import sys
import requests
import json
import os
from typing import List, Dict, Any
import subprocess

def check_ollama_installation() -> bool:
    """V√©rifie si Ollama est install√©."""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(f"‚úÖ Ollama install√© : {result.stdout.strip()}")
            return True
        else:
            print("‚ùå Ollama n'est pas install√© ou pas dans le PATH")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError):
        print("‚ùå Ollama n'est pas install√© ou pas accessible")
        return False

def check_ollama_server(host: str = "http://localhost:11434") -> bool:
    """V√©rifie si le serveur Ollama est en cours d'ex√©cution."""
    try:
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            print(f"‚úÖ Serveur Ollama actif sur {host}")
            return True
        else:
            print(f"‚ùå Serveur Ollama non accessible (statut: {response.status_code})")
            return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Impossible de se connecter au serveur Ollama : {e}")
        return False

def list_available_models(host: str = "http://localhost:11434") -> List[Dict[str, Any]]:
    """Liste les mod√®les Ollama disponibles."""
    try:
        response = requests.get(f"{host}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])
            if models:
                print(f"‚úÖ {len(models)} mod√®le(s) Ollama disponible(s) :")
                for model in models:
                    name = model.get("name", "Unknown")
                    size = model.get("size", 0)
                    size_gb = size / (1024**3) if size > 0 else 0
                    print(f"  - {name} ({size_gb:.1f} GB)")
            else:
                print("‚ö†Ô∏è Aucun mod√®le Ollama install√©")
            return models
        else:
            print(f"‚ùå Erreur lors de la r√©cup√©ration des mod√®les : {response.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur de connexion lors de la r√©cup√©ration des mod√®les : {e}")
        return []

def test_model_generation(model_name: str, host: str = "http://localhost:11434") -> bool:
    """Teste la g√©n√©ration de texte avec un mod√®le."""
    try:
        payload = {
            "model": model_name,
            "prompt": "Bonjour, peux-tu te pr√©senter bri√®vement en fran√ßais ?",
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 100
            }
        }
        
        print(f"üß™ Test de g√©n√©ration avec le mod√®le {model_name}...")
        response = requests.post(f"{host}/api/generate", json=payload, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            generated_text = data.get("response", "").strip()
            if generated_text:
                print(f"‚úÖ G√©n√©ration r√©ussie :")
                print(f"   {generated_text[:200]}...")
                return True
            else:
                print("‚ùå Aucun texte g√©n√©r√©")
                return False
        else:
            print(f"‚ùå Erreur de g√©n√©ration : {response.status_code}")
            print(f"   {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors du test de g√©n√©ration : {e}")
        return False

def check_env_configuration() -> Dict[str, str]:
    """V√©rifie la configuration du fichier .env."""
    env_path = ".env"
    config = {}
    
    if os.path.exists(env_path):
        print(f"‚úÖ Fichier .env trouv√©")
        try:
            with open(env_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        config[key.strip()] = value.strip()
            
            ollama_host = config.get('OLLAMA_HOST', 'Non configur√©')
            ollama_model = config.get('OLLAMA_MODEL', 'Non configur√©')
            
            print(f"  - OLLAMA_HOST: {ollama_host}")
            print(f"  - OLLAMA_MODEL: {ollama_model}")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la lecture du .env : {e}")
    else:
        print("‚ö†Ô∏è Fichier .env non trouv√©")
        
    return config

def suggest_model_installation():
    """Sugg√®re l'installation d'un mod√®le si aucun n'est disponible."""
    print("\nüì• Suggestions d'installation de mod√®les :")
    print("\nMod√®les recommand√©s :")
    print("  1. Pour usage g√©n√©ral : ollama pull llama3.1")
    print("  2. Pour machines modestes : ollama pull phi3:mini")
    print("  3. Pour l'analyse de code : ollama pull codellama")
    print("\nPour installer un mod√®le :")
    print("  ollama pull <nom_du_modele>")

def suggest_server_start():
    """Sugg√®re comment d√©marrer le serveur Ollama."""
    print("\nüöÄ Pour d√©marrer le serveur Ollama :")
    print("  ollama serve")
    print("\nOu en arri√®re-plan :")
    print("  nohup ollama serve &")

def main():
    """Fonction principale de v√©rification."""
    print("üîç V√©rification de la configuration Ollama")
    print("=" * 50)
    
    # V√©rification de l'installation
    ollama_installed = check_ollama_installation()
    
    # Configuration depuis .env
    config = check_env_configuration()
    host = config.get('OLLAMA_HOST', 'http://localhost:11434')
    preferred_model = config.get('OLLAMA_MODEL', 'llama3.1')
    
    # V√©rification du serveur
    server_running = check_ollama_server(host)
    
    if not server_running and ollama_installed:
        suggest_server_start()
        return
    
    # Liste des mod√®les
    models = list_available_models(host) if server_running else []
    
    if not models and server_running:
        suggest_model_installation()
        return
    
    # Test du mod√®le pr√©f√©r√©
    if models and server_running:
        model_names = [model["name"] for model in models]
        
        # Chercher le mod√®le pr√©f√©r√©
        test_model = None
        for model_name in model_names:
            if preferred_model in model_name:
                test_model = model_name
                break
        
        # Si pas trouv√©, utiliser le premier disponible
        if not test_model:
            test_model = model_names[0]
            print(f"‚ö†Ô∏è Mod√®le pr√©f√©r√© '{preferred_model}' non trouv√©, utilisation de '{test_model}'")
        
        # Test de g√©n√©ration
        generation_ok = test_model_generation(test_model, host)
        
        print("\n" + "=" * 50)
        print("üìä R√âSUM√â DE LA V√âRIFICATION")
        print("=" * 50)
        print(f"Ollama install√© : {'‚úÖ' if ollama_installed else '‚ùå'}")
        print(f"Serveur actif : {'‚úÖ' if server_running else '‚ùå'}")
        print(f"Mod√®les disponibles : {'‚úÖ' if models else '‚ùå'}")
        print(f"G√©n√©ration de texte : {'‚úÖ' if generation_ok else '‚ùå'}")
        
        if ollama_installed and server_running and models and generation_ok:
            print("\nüéâ Configuration Ollama parfaite ! Votre chatbot est pr√™t.")
        else:
            print("\n‚ö†Ô∏è Configuration incompl√®te. Consultez le guide OLLAMA_SETUP.md")

if __name__ == "__main__":
    main()
